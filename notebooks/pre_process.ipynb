{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Knowledge Graph Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21354359it [01:16, 280219.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "triplets = {}\n",
    "# Function to convert text file to JSONL\n",
    "def convert_text_to_jsonl(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "            # Assuming the ID and description are separated by a space\n",
    "            # Modify the delimiter as needed\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                h, r, t = parts\n",
    "                if not triplets.get(h):\n",
    "                    triplets[h] = [[h,r,t]]\n",
    "                else:\n",
    "                    triplets[h].append([h,r,t])\n",
    "        for key in triplets:\n",
    "            json_obj = {'key': key, 'triplets': triplets[key]}\n",
    "            outfile.write(json.dumps(json_obj) + '\\n')\n",
    "\n",
    "# Replace 'input.txt' with your input file name\n",
    "# Replace 'output.jsonl' with your desired output file name\n",
    "convert_text_to_jsonl('/home/zho1rng/oven_train/dataset/wikidata5m/wikidata5m_all_triplet.txt', '/home/zho1rng/oven_train/dataset/wikidata5m/wikidata5m_all_triplet.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20549\n",
      "4799312\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "entity_set = set()\n",
    "\n",
    "files = ['/home/zho1rng/oven_train/dataset/oven_data/oven_entity_train.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_entity_val.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_query_train.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_query_val.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_entity_test.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_query_test.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_human.jsonl']\n",
    "\n",
    "for path in files:\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            json_object = json.loads(line)\n",
    "            q_id = json_object['entity_id']\n",
    "            entity_set.add(q_id)\n",
    "\n",
    "print(len(entity_set))\n",
    "\n",
    "            \n",
    "with open('missing_entity.txt', 'w') as output:\n",
    "    for e in entity_set:\n",
    "        if not triplets.get(e):\n",
    "            output.write(e + '\\n')\n",
    "    \n",
    "print(len(triplets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oven_entity.txt', 'w') as output:\n",
    "    for e in entity_set:\n",
    "            output.write(e + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikidata5m_entity.txt', 'w') as output:\n",
    "    for key in triplets:\n",
    "            output.write(key + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "with open('/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_relation.txt', 'r') as infile, open('/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_relation+.txt', 'w') as outfile:\n",
    "    for f in tqdm(infile):\n",
    "        words = f.split()\n",
    "        outfile.writelines(words[0] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplets with head index to triplets with tail index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4820473it [00:02, 2399060.86it/s]\n",
      "4801546it [01:21, 58764.59it/s] \n",
      "100%|██████████| 4820473/4820473 [00:18<00:00, 259212.30it/s] \n",
      "100%|██████████| 21402665/21402665 [00:16<00:00, 1278322.02it/s]\n",
      "100%|██████████| 4820473/4820473 [00:23<00:00, 207048.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "triplets_path = '/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_all_triplet+.jsonl'\n",
    "triplets_t_path = '/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_triplet_t+.jsonl'\n",
    "triplets_h_path = '/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_triplet_h+.jsonl'\n",
    "entity_path = '/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_entity++.txt'\n",
    "triplets_list = []\n",
    "entity_set = set()\n",
    "with open(entity_path, 'r') as infile:\n",
    "    for line in tqdm(infile):\n",
    "        entity_set.add(line[:-1])\n",
    "\n",
    "with open(triplets_path, 'r') as infile, open(triplets_h_path, 'w') as outfile:\n",
    "    for line in tqdm(infile):\n",
    "        json_object = json.loads(line)\n",
    "        for i in json_object['triplets']:\n",
    "            triplets_list.append(i)\n",
    "        if json_object['triplets']!=[]:\n",
    "            outfile.write(json.dumps(json_object) + '\\n')\n",
    "\n",
    "triplets_dict = {}\n",
    "for entity in tqdm(entity_set):\n",
    "    triplets_dict[entity] = []\n",
    "    \n",
    "for triplet in tqdm(triplets_list):\n",
    "    h = triplet[0]\n",
    "    r = triplet[1]\n",
    "    t= triplet[2]\n",
    "    triplets_dict[t].append(triplet)\n",
    "\n",
    "with open(triplets_t_path, 'w') as outfile:\n",
    "    for key in tqdm(triplets_dict):\n",
    "        if triplets_dict[key]!=[]:\n",
    "            json_obj = {'key': key, 'triplets': triplets_dict[key]}\n",
    "            outfile.write(json.dumps(json_obj) + '\\n')\n",
    "# with open(entity_path_n, 'w') as outfile:\n",
    "#     for e in tqdm(entity_set):\n",
    "#         outfile.writelines(e + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sub-graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4926314it [00:16, 304296.58it/s]\n",
      "126199it [00:00, 303692.89it/s]\n",
      "32255it [00:00, 297952.19it/s]\n",
      "3291it [00:00, 291575.05it/s]\n",
      "709478it [00:02, 300281.97it/s]\n",
      "19781it [00:00, 298799.03it/s]\n",
      "24867it [00:00, 299714.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "825it [00:00, 1283970.61it/s]\n",
      "4801110it [00:51, 93453.83it/s] \n",
      "153007it [00:19, 7767.31it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_triplet_t+.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(infile):\n\u001b[0;32m---> 40\u001b[0m         json_object \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         triplets_t_dict[json_object[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m json_object[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriplets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "kg_path = '../dataset/wikidata5m+/'\n",
    "sub_kg_path = '../dataset/wikidata5m+_1hop'\n",
    "import json\n",
    "oven_entity_set = set()\n",
    "\n",
    "files = ['/home/zho1rng/oven_train/dataset/oven_data/oven_entity_train.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_entity_val.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_query_train.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_query_val.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_entity_test.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_query_test.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_human.jsonl']\n",
    "\n",
    "for path in files:\n",
    "    with open(path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            json_object = json.loads(line)\n",
    "            q_id = json_object['entity_id']\n",
    "            oven_entity_set.add(q_id)\n",
    "\n",
    "print(len(oven_entity_set))\n",
    "\n",
    "\n",
    "rel_set = set()\n",
    "with open('/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_relation+.txt', 'r') as infile:\n",
    "    for line in tqdm(infile):\n",
    "        rel_set.add(line[:-1])\n",
    "        \n",
    "triplets_h_dict = {}\n",
    "triplets_t_dict = {}\n",
    "\n",
    "with open('/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_triplet_h+.jsonl', 'r') as infile:\n",
    "    for line in tqdm(infile):\n",
    "        json_object = json.loads(line)\n",
    "        triplets_h_dict[json_object['key']] = json_object['triplets']\n",
    "\n",
    "with open('/home/zho1rng/oven_train/dataset/wikidata5m+/wikidata5m_triplet_t+.jsonl', 'r') as infile:\n",
    "    for line in tqdm(infile):\n",
    "        json_object = json.loads(line)\n",
    "        triplets_t_dict[json_object['key']] = json_object['triplets']\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3506363\n"
     ]
    }
   ],
   "source": [
    "hop1_entity_set = set()\n",
    "\n",
    "\n",
    "for e in oven_entity_set:\n",
    "    if e in triplets_h_dict:\n",
    "        for triplet in triplets_h_dict[e]:\n",
    "            hop1_entity_set.add(triplet[0])\n",
    "            hop1_entity_set.add(triplet[2])\n",
    "    if e in triplets_t_dict:\n",
    "        for triplet in triplets_t_dict[e]:\n",
    "            hop1_entity_set.add(triplet[0])\n",
    "            hop1_entity_set.add(triplet[2])\n",
    "\n",
    "print(len(hop1_entity_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-hop + hierachical info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20549/20549 [00:00<00:00, 136725.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P360 common element between all listed items\n",
      "P495 country of origin of this item (creative work, food, phrase, product, etc.)\n",
      "P832 official public holiday that occurs in this place in its honor, usually a non-working day\n",
      "P2012 type of food served by a restaurant or restaurant chain or national food culture\n",
      "P411 Formal stage in the process of attaining sainthood per the subject's religious organization\n",
      "P451 someone with whom the person is in a relationship without being married. Use \"spouse\" (P26) for married couples\n",
      "P156 immediately following item in a series of which the subject is a part, preferably use as qualifier of P179 [if the subject has been replaced, e.g. political offices, use \"replaced by\" (P1366)]\n",
      "P1552 inherent or distinguishing quality or feature of the entity. Use a more specific property when possible.\n",
      "P3113 expected part that the item does not have (for qualities, use P6477)\n",
      "P551 the place where the person is or has been, resident\n",
      "P825 person or organization to whom the subject was dedicated\n",
      "P740 location where a group or organization was formed\n",
      "P397 major astronomical body the item belongs to\n",
      "P400 platform for which a work was developed or released, or the specific platform version of a software product\n",
      "P1366 other person or item which continues the item by replacing it in its role. Use P156 (\"followed by\") if the item is not replaced nor identical, but adds to the series (e.g. books in a series).\n",
      "P517 subset of the four fundamental forces (strong (Q11415), electromagnetic (Q849919), weak (Q11418), and gravitation (Q11412) with which a particle interacts\n",
      "P136 creative work's genre or an artist's field of work (P101). Use main subject (P921) to relate creative works to their topic\n",
      "P2414 substrate that an enzyme acts upon to create a product\n",
      "P1336 administrative divisions that claim control of a given area\n",
      "P6 head of the executive power of this town, city, municipality, state, country, or other governmental body\n",
      "P47 countries or administrative subdivisions, of equal level, that this item borders, either by land or water. A single common point is enough.\n",
      "P88 person or organization that commissioned this work\n",
      "P115 home stadium or venue of a sports team or applicable performing arts organization\n",
      "P1071 place where the item was conceived or made; where applicable, location of final assembly\n",
      "P2283 item or concept used by the subject or in the operation (see also instrument [P1303] and armament [P520])\n",
      "P398 minor body that belongs to the item\n",
      "P364 language in which a film or a performance work was originally created. Deprecated for written works and songs; use P407 (\"language of work or name\") instead.\n",
      "P689 type of organism which a condition or disease afflicts (for an organ affected by a condition, use P927)\n",
      "P1056 material or product produced by an organization, industry, facility, or process\n",
      "P937 location where persons or organisations were actively participating in employment, business or other work\n",
      "P205 country that have drainage to/from or border the body of water\n",
      "P1001 the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...)\n",
      "P201 rivers and other outflows waterway names. If evaporation or seepage are notable outflows, they may be included. Some terms may not be place names, e.g. evaporation\n",
      "P2579 subject is studied by this science or domain\n",
      "P1313 political office that is fulfilled by the head of the government of this item\n",
      "P170 maker of this creative work or other object (where no more specific property exists)\n",
      "P3902 components of the last meal had by a person before death\n",
      "P462 color of subject\n",
      "P1165 home planet or natural satellite for a fictional character or species\n",
      "P1050 any state relevant to the health of an organism, including diseases and positive conditions\n",
      "P355 subsidiary of a company or organization; generally a fully owned separate corporation. Compare with \"business division\" (P199). Opposite of parent organization (P749).\n",
      "P3512 method that the subject uses to move from one place to another\n",
      "P149 architectural style of a structure\n",
      "P161 actor in the subject production [use \"character role\" (P453) and/or \"name of the character role\" (P4633) as qualifiers] [use \"voice actor\" (P725) for voice-only role]\n",
      "P131 the item is located on the territory of the following administrative entity. Use P276 for specifying locations that are non-administrative places and for items about events. Use P1382 if the item falls only partially into the administrative entity.\n",
      "P287 person(s) or organization which designed the object\n",
      "P2575 physical quantity that this device measures\n",
      "P1303 musical instrument that a person plays or teaches or used in a music occupation\n",
      "P2789 item with which the item is physically connected\n",
      "P30 continent of which the subject is a part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sub_rel_set= set()\n",
    "for e in tqdm(oven_entity_set):\n",
    "    if e in triplets_h_dict:\n",
    "        for triplet in triplets_h_dict[e]:\n",
    "            if triplet[2] in oven_entity_set:\n",
    "                sub_rel_set.add(triplet[1])\n",
    "    if e in triplets_t_dict:\n",
    "        for triplet in triplets_t_dict[e]:\n",
    "            if triplet[0] in oven_entity_set:\n",
    "                sub_rel_set.add(triplet[1])\n",
    "                \n",
    "rel_text_dict = {}\n",
    "with open('/home/zho1rng/oven_train/dataset/knowledge_base/wikidata_relation.jsonl') as infile:\n",
    "    for line in infile:\n",
    "        json_obj = json.loads(line)\n",
    "        rel_text_dict[json_obj['p_id']] =json_obj['description']\n",
    "\n",
    "k=50\n",
    "for rel in sub_rel_set:\n",
    "    print(rel, rel_text_dict[rel])\n",
    "    k-=1\n",
    "    if k ==0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20549/20549 [00:00<00:00, 110831.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oven_entity_set_plus = oven_entity_set.copy()\n",
    "for e in tqdm(oven_entity_set):\n",
    "    start = e\n",
    "    flag = True\n",
    "    n = 0\n",
    "    while flag and n<20:\n",
    "        flag = False\n",
    "        n+=1\n",
    "        if start in triplets_h_dict:\n",
    "            for triplet in triplets_h_dict[start]:\n",
    "                if triplet[1] == 'P31' or triplet[1] == 'P279':\n",
    "                    start = triplet[2]\n",
    "                    oven_entity_set_plus.add(triplet[2])\n",
    "                    flag = True\n",
    "                    break\n",
    "\n",
    "print(len(oven_entity_set_plus))\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_path = '/home/zho1rng/oven_train/dataset/wikidata5m_s/entity.txt'\n",
    "rel_path = '/home/zho1rng/oven_train/dataset/wikidata5m_s/relation.txt'\n",
    "\n",
    "with open(entity_path, 'w') as outfile:\n",
    "    for e in oven_entity_set_plus:\n",
    "        outfile.writelines(e + '\\n')\n",
    "\n",
    "with open(rel_path, 'w') as outfile:\n",
    "    for r in sub_rel_set:\n",
    "        outfile.writelines(r + '\\n')\n",
    "        \n",
    "triplets_h_path = '/home/zho1rng/oven_train/dataset/wikidata5m_s/triplet_h.jsonl'\n",
    "triplets_t_path = '/home/zho1rng/oven_train/dataset/wikidata5m_s/triplet_t.jsonl'\n",
    "\n",
    "sub_triplets_h_dict = {}\n",
    "sub_triplets_t_dict = {}\n",
    "for key in triplets_h_dict:\n",
    "    for triplet in triplets_h_dict[key]:\n",
    "        if triplet[0] in oven_entity_set_plus and triplet[2] in oven_entity_set_plus and triplet[1] in sub_rel_set:\n",
    "            if not triplet[0] in sub_triplets_h_dict:\n",
    "                sub_triplets_h_dict[triplet[0]] = []\n",
    "            else:\n",
    "                sub_triplets_h_dict[triplet[0]].append(triplet)\n",
    "            \n",
    "            if not triplet[2] in sub_triplets_t_dict:\n",
    "                sub_triplets_t_dict[triplet[2]] = []\n",
    "            else:\n",
    "                sub_triplets_t_dict[triplet[2]].append(triplet)\n",
    "\n",
    "with open(triplets_h_path, 'w') as outfile:\n",
    "    for key in sub_triplets_h_dict:\n",
    "        if sub_triplets_h_dict[key] != []:\n",
    "            json_object = {'key': key, 'triplets':sub_triplets_h_dict[key]}\n",
    "            outfile.writelines(json.dumps(json_object) + '\\n')\n",
    "        \n",
    "with open(triplets_t_path, 'w') as outfile:\n",
    "    for key in sub_triplets_t_dict:\n",
    "        if sub_triplets_t_dict[key] != []:\n",
    "            json_object = {'key': key, 'triplets':sub_triplets_t_dict[key]}\n",
    "            outfile.writelines(json.dumps(json_object) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Knowledge Base Pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6084491it [04:57, 20466.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open('/home/zho1rng/oven_train/dataset/knowledge_base/Wiki6M_ver_1_0.jsonl', 'r') as infile, open('/home/zho1rng/oven_train/dataset/knowledge_base/Wiki6M_ver_1_1.jsonl', 'w') as outfile:\n",
    "    for f in tqdm(infile):\n",
    "        json_object = json.loads(f)\n",
    "        json_object_n = {'wikidata_id': json_object['wikidata_id'], 'wikipedia_title':json_object['wikipedia_title'], 'wikipedia_summary': json_object['wikipedia_summary']}\n",
    "        outfile.writelines(json.dumps(json_object_n) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "with open('/home/zho1rng/oven_train/dataset/knowledge_base/Wiki6M_ver_1_0.jsonl', 'r') as infile, open('/home/zho1rng/oven_train/dataset/knowledge_base/Wiki6M_ver_1_1.jsonl', 'w') as outfile:\n",
    "    for f in tqdm(infile):\n",
    "        json_object = json.loads(f)\n",
    "        json_object_n = {'wikidata_id': json_object['wikidata_id'], 'wikipedia_title':json_object['wikipedia_title'], 'wikipedia_summary': json_object['wikipedia_summary']}\n",
    "        outfile.writelines(json.dumps(json_object_n) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "825it [00:00, 170189.89it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph Embedding Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = ['.JPEG', '.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']\n",
    "from knowcol.datasets.utils import pil_loader\n",
    "\n",
    "def load_image(self, img_id):\n",
    "    for ext in extensions:\n",
    "        try:\n",
    "            img_path = self.imgid2path(img_id, ext)\n",
    "            return pil_loader(img_path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    assert('No image with this id!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from knowcol.datasets.utils import pil_loader\n",
    "from pathlib import Path\n",
    "\n",
    "entity_set = set()\n",
    "\n",
    "def qid2img( qid: str):\n",
    "    id = int(qid[1:])\n",
    "    if id < 100:\n",
    "        return (Path('/home/zho1rng/oven_train/dataset/knowledge_base') / 'wikipedia_images_full' / qid / f'{qid}.jpg').as_posix()\n",
    "    else:\n",
    "        return (Path('/home/zho1rng/oven_train/dataset/knowledge_base') / 'wikipedia_images_full' / qid[:4] / f'{qid}.jpg').as_posix()\n",
    "\n",
    "\n",
    "files = [\n",
    "        '/home/zho1rng/oven_train/dataset/oven_data/oven_entity_train.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_entity_val.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_query_train.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/oven_data/oven_query_val.jsonl',\n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_entity_test.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_query_test.jsonl', \n",
    "         '/home/zho1rng/oven_train/dataset/test_data/oven_human.jsonl'\n",
    "        ]\n",
    "\n",
    "for path in files:\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            json_object = json.loads(line)\n",
    "            q_id = json_object['entity_id']\n",
    "            entity_set.add(q_id)\n",
    "\n",
    "l = []\n",
    "for q_id in entity_set:\n",
    "    image_path = qid2img(q_id)\n",
    "    if not Path(image_path).is_file():\n",
    "        l.append(q_id)\n",
    "print(len(l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: proxy_on: command not found\n"
     ]
    }
   ],
   "source": [
    "!module load proxy4server-access/ & proxy_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /wiki/Special:EntityData/Q42.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f209a061fc0>: Failed to resolve 'www.wikidata.org' ([Errno -2] Name or service not known)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f209a061fc0>: Failed to resolve 'www.wikidata.org' ([Errno -2] Name or service not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /wiki/Special:EntityData/Q42.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f209a061fc0>: Failed to resolve 'www.wikidata.org' ([Errno -2] Name or service not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m qid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ42\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your desired QID\u001b[39;00m\n\u001b[1;32m     50\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouglas_adams.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# File name to save the image\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_wikidata_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mdownload_wikidata_image\u001b[0;34m(qid, save_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step 1: Get the JSON data for the QID\u001b[39;00m\n\u001b[1;32m     16\u001b[0m api_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.wikidata.org/wiki/Special:EntityData/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data for QID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. HTTP Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.conda/envs/oven/lib/python3.10/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /wiki/Special:EntityData/Q42.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f209a061fc0>: Failed to resolve 'www.wikidata.org' ([Errno -2] Name or service not known)\"))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_wikidata_image(qid, save_path=\"downloaded_image.jpg\"):\n",
    "    \"\"\"\n",
    "    Download the main image (P18) of a Wikidata entity (QID) and save it locally.\n",
    "    \n",
    "    Args:\n",
    "        qid (str): Wikidata QID (e.g., \"Q42\").\n",
    "        save_path (str): Path to save the downloaded image.\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved image or an error message.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the JSON data for the QID\n",
    "    api_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return f\"Failed to fetch data for QID {qid}. HTTP Status: {response.status_code}\"\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    try:\n",
    "        # Step 2: Extract the P18 property (image name)\n",
    "        image_name = data['entities'][qid]['claims']['P18'][0]['mainsnak']['datavalue']['value']\n",
    "        \n",
    "        # Step 3: Construct the image URL\n",
    "        image_name_encoded = image_name.replace(\" \", \"_\")\n",
    "        commons_base_url = \"https://upload.wikimedia.org/wikipedia/commons\"\n",
    "        hash_prefix = f\"{image_name_encoded[0]}/{image_name_encoded[0:2]}\"\n",
    "        image_url = f\"{commons_base_url}/{hash_prefix}/{image_name_encoded}\"\n",
    "        \n",
    "        # Step 4: Download the image\n",
    "        img_response = requests.get(image_url, stream=True)\n",
    "        \n",
    "        if img_response.status_code == 200:\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                for chunk in img_response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            return f\"Image downloaded successfully and saved to {save_path}\"\n",
    "        else:\n",
    "            return f\"Failed to download image. HTTP Status: {img_response.status_code}\"\n",
    "    \n",
    "    except KeyError:\n",
    "        return f\"No image (P18 property) found for QID {qid}.\"\n",
    "\n",
    "# Example usage\n",
    "qid = \"Q42\"  # Replace with your desired QID\n",
    "save_path = \"douglas_adams.jpg\"  # File name to save the image\n",
    "result = download_wikidata_image(qid, save_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Entity Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from knowcol.datasets.kg import KG\n",
    "from knowcol.datasets.data_module import _transform\n",
    "from knowcol.datasets.utils import _load_image_from_path\n",
    "\n",
    "clip_model = 'ViT-L-14'\n",
    "pretrained = 'commonpool_xl_s13b_b90k'\n",
    "\n",
    "knowledge_base_path='dataset/knowledge_base'\n",
    "entity_set_path='dataset/wikidata5m_s/entity.txt'\n",
    "relation_path='dataset/wikidata5m_s/relation.txt'\n",
    "triplet_h_path='dataset/wikidata5m_s/triplet_h.jsonl'\n",
    "triplet_t_path='dataset/wikidata5m_s/triplet_t.jsonl'\n",
    "N_px = 224\n",
    "device = 'cuda'\n",
    "kg = KG(\n",
    "        knowledge_base_path=knowledge_base_path, \n",
    "        entity_set_path=entity_set_path, \n",
    "        relation_path=relation_path,\n",
    "        triplet_h_path=triplet_h_path,\n",
    "        triplet_t_path=triplet_t_path\n",
    "        )\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(clip_model, pretrained=pretrained)\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer(clip_model)\n",
    "\n",
    "\n",
    "\n",
    "entity_embs = []\n",
    "entity_summaries = kg.get_ents_text(kg.entities)\n",
    "entity_imgs_path = kg.get_ents_image(kg.entities)       \n",
    "\n",
    "\n",
    "for i in tqdm(range(kg.n_ent)):\n",
    "    with torch.no_grad():\n",
    "        entity_summary_tokenized = tokenizer(entity_summaries[i]).squeeze().to(device)\n",
    "        entity_image_path = entity_imgs_path[i]\n",
    "        entity_image, has_entity_img = _load_image_from_path(entity_image_path)\n",
    "        entity_image = _transform(N_px)(entity_image).to(device)\n",
    "        \n",
    "        entity_image = entity_image[None, ...]\n",
    "        entity_summary_tokenized = entity_summary_tokenized[None, ...]\n",
    "        entity_image_emb = model.encode_image(entity_image) # (B, N_z)\n",
    "        entity_text_emb = model.encode_text(entity_summary_tokenized) # (B, N_z)\n",
    "        entity_image_emb = entity_image_emb * has_entity_img + entity_text_emb * (1.0 - has_entity_img) # replace image emb. of the entities without sample image with its text emb\n",
    "\n",
    "        entity_emb = entity_text_emb + entity_image_emb\n",
    "        entity_embs.append(entity_emb)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pdb\n",
    "\n",
    "def load_list_from_txt(file_path):\n",
    "    \"\"\"\n",
    "    Loads a list from a text file, where each line in the file is an item in the list.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings read from the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return [line.strip() for line in file]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def get_wikipedia_image_by_qid(qid):\n",
    "    \"\"\"\n",
    "    Fetches the image URL associated with a Wikidata QID.\n",
    "    Args:\n",
    "        qid (str): The Wikidata QID (e.g., \"Q243\" for the Eiffel Tower).\n",
    "    Returns:\n",
    "        str: The image URL or None if not found.\n",
    "    \"\"\"\n",
    "    # Construct the API URL\n",
    "    wikidata_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\n",
    "    }\n",
    "    # Fetch the data from Wikidata\n",
    "    response = requests.get(wikidata_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        claims = data.get(\"entities\", {}).get(qid, {}).get(\"claims\", {})\n",
    "        if \"P18\" in claims:  # P18 is the property for images\n",
    "            image_name = claims[\"P18\"][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "            return f\"https://commons.wikimedia.org/wiki/Special:FilePath/{image_name.replace(' ', '_')}\"\n",
    "    return None\n",
    "\n",
    "def download_image(image_url, save_path):\n",
    "    \"\"\"\n",
    "    Downloads an image from a given URL.\n",
    "    Args:\n",
    "        image_url (str): The URL of the image.\n",
    "        save_path (str): The file path to save the image.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(image_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Image saved to {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download the image. Status code: {response.status_code}\")\n",
    "\n",
    "loaded_list = load_list_from_txt('qids.txt')\n",
    "for qid in loaded_list:\n",
    "    image_url = get_wikipedia_image_by_qid(qid)\n",
    "\n",
    "    if image_url:\n",
    "        print(f\"Image URL: {image_url}\")\n",
    "        download_image(image_url, f\"images/{qid}.jpg\")\n",
    "    else:\n",
    "        print(f\"No image found for the given QID {qid}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
