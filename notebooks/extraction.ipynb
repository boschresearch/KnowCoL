{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a93a40c",
   "metadata": {},
   "source": [
    "## Extract sub-KG\n",
    "1. add all superclasses\n",
    "2. relations among them\n",
    "3. add transitivity relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505a5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ancestors:   0%|          | 0/20549 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 61] Connection refused>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1355\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    923\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vf/9pm0jjr11zl8xwfn0vcf3wmm0000gp/T/ipykernel_51213/3331187115.py\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mall_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_qids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_qids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Fetching ancestors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mall_entities\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mget_all_ancestors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Write entity list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vf/9pm0jjr11zl8xwfn0vcf3wmm0000gp/T/ipykernel_51213/3331187115.py\u001b[0m in \u001b[0;36mget_all_ancestors\u001b[0;34m(qid, depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mancestor_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mancestor_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mparents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_direct_parents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mQID_PATTERN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vf/9pm0jjr11zl8xwfn0vcf3wmm0000gp/T/ipykernel_51213/3331187115.py\u001b[0m in \u001b[0;36mget_direct_parents\u001b[0;34m(qid)\u001b[0m\n\u001b[1;32m     49\u001b[0m     q = f\"\"\"\n\u001b[1;32m     50\u001b[0m     SELECT ?parent WHERE {{ wd:{qid} (wdt:P31|wdt:P279) ?parent . }}\"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparql_query_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bindings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vf/9pm0jjr11zl8xwfn0vcf3wmm0000gp/T/ipykernel_51213/3331187115.py\u001b[0m in \u001b[0;36msparql_query_with_retry\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_RETRIES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msparql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_RETRIES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/SPARQLWrapper/Wrapper.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mQueryResult\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \"\"\"\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mQueryResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueryAndConvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"QueryResult.ConvertResult\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/SPARQLWrapper/Wrapper.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnFormat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    543\u001b[0m                                   '_open', req)\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1398\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 61] Connection refused>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import socket\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from urllib.error import HTTPError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global socket timeout for HTTP operations\n",
    "socket.setdefaulttimeout(30)\n",
    "\n",
    "# Constants\n",
    "OUTPUT_DIR          = 'kg'\n",
    "INPUT_FILE          = 'oven_entity.txt'\n",
    "QID_PATTERN         = re.compile(r\"^Q\\d+$\")\n",
    "ANCESTOR_CHUNK_SIZE = 20    # QIDs per batch for ancestor fetch\n",
    "RELATION_CHUNK_SIZE = 50    # Entities per batch for relation fetch\n",
    "TRANS_CHUNK_SIZE    = 50    # Entities per batch for transitive property fetch\n",
    "MAX_WORKERS         = 8     # Parallel threads\n",
    "MAX_RETRIES         = 10     # Retry attempts\n",
    "BASE_BACKOFF        = 2     # Base seconds for exponential backoff\n",
    "\n",
    "# List of truly transitive properties to include\n",
    "TRANSITIVE_PROPS = [\n",
    "    'P31', 'P279',  # instance/subclass\n",
    "    'P131',        # located in the administrative entity\n",
    "    'P361',        # part of\n",
    "    'P171',        # parent taxon\n",
    "    'P155', 'P156',# follows / followed by\n",
    "    'P1365', 'P1366', # see also / has edition\n",
    "    'P3729', 'P3730', # first/last appearance\n",
    "    'P5135', 'P5136'  # category topics\n",
    "]\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Read and validate input QIDs\n",
    "with open(INPUT_FILE) as f:\n",
    "    raw_qids = [line.strip() for line in f if line.strip()]\n",
    "input_qids = [qid for qid in dict.fromkeys(raw_qids) if QID_PATTERN.match(qid)]\n",
    "if not input_qids:\n",
    "    raise ValueError(\"No valid QIDs found in input file.\")\n",
    "\n",
    "# Helper: split list into chunks\n",
    "def chunk_list(lst, size):\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i:i+size]\n",
    "\n",
    "# Helper: perform SPARQL query with retry/backoff and timeout\n",
    "def sparql_query(client, query):\n",
    "    client.setQuery(query)\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            return client.query().convert()\n",
    "        except (HTTPError, socket.timeout) as e:\n",
    "            code = e.code if isinstance(e, HTTPError) else 'timeout'\n",
    "            if attempt == MAX_RETRIES:\n",
    "                tqdm.write(f\"[{code}] Failed after {attempt} attempts: {e}\")\n",
    "                raise\n",
    "            backoff = BASE_BACKOFF * (2 ** (attempt - 1)) + random.random()\n",
    "            tqdm.write(f\"[{code}] Retrying in {backoff:.1f}s...\")\n",
    "            time.sleep(backoff)\n",
    "\n",
    "# 1️⃣ Batch + threaded ancestor fetching\n",
    "#    Only fetch direct subclass(*) or one-step instanceOf then subclass(*)\n",
    "all_entities = set(input_qids)\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {}\n",
    "    for chunk in chunk_list(input_qids, ANCESTOR_CHUNK_SIZE):\n",
    "        vals = ' '.join(f'wd:{qid}' for qid in chunk)\n",
    "        # Use union of subclassOf* or instanceOf followed by subclassOf*\n",
    "        query = f\"\"\"\n",
    "SELECT DISTINCT ?ancestor WHERE {{\n",
    "  VALUES ?item {{ {vals} }}\n",
    "  ?item (wdt:P279*|wdt:P31/wdt:P279*) ?ancestor .\n",
    "}}\"\"\"\n",
    "        client = SPARQLWrapper('https://query.wikidata.org/sparql')\n",
    "        client.setReturnFormat(JSON)\n",
    "        futures[executor.submit(sparql_query, client, query)] = chunk\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc='Ancestor batches'):\n",
    "        try:\n",
    "            res = fut.result(timeout=60)\n",
    "            for b in res['results']['bindings']:\n",
    "                qid = b['ancestor']['value'].split('/')[-1]\n",
    "                if QID_PATTERN.match(qid):\n",
    "                    all_entities.add(qid)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Ancestor chunk error {futures[fut]}: {e}\")\n",
    "\n",
    "# Write entities\n",
    "with open(os.path.join(OUTPUT_DIR, 'entity.txt'), 'w') as f:\n",
    "    for qid in tqdm(sorted(all_entities), desc='Writing entity.txt'):\n",
    "        f.write(qid + '\\n')\n",
    "\n",
    "# 2️⃣ Prepare for deduplicated triplet collection\n",
    "triplets_out = {}\n",
    "triplets_in  = {}\n",
    "triplets_out_seen = {}\n",
    "triplets_in_seen  = {}\n",
    "relation_ids = set()\n",
    "\n",
    "# Helper: add a triplet if not already seen\n",
    "def add_triplet(out_dict, seen_dict, subj, prop, obj):\n",
    "    seen = seen_dict.setdefault(subj, set())\n",
    "    if (subj, prop, obj) not in seen:\n",
    "        seen.add((subj, prop, obj))\n",
    "        out_dict.setdefault(subj, []).append([subj, prop, obj])\n",
    "\n",
    "# 2️⃣ Batch + threaded direct relation fetching\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {}\n",
    "    for chunk in chunk_list(sorted(all_entities), RELATION_CHUNK_SIZE):\n",
    "        vals = ' '.join(f'wd:{e}' for e in chunk)\n",
    "        query = f\"\"\"\n",
    "SELECT ?s ?p ?o WHERE {{\n",
    "  VALUES ?s {{ {vals} }}\n",
    "  ?s ?prop ?o .\n",
    "  ?p wikibase:directClaim ?prop .\n",
    "}}\"\"\"\n",
    "        client = SPARQLWrapper('https://query.wikidata.org/sparql')\n",
    "        client.setReturnFormat(JSON)\n",
    "        futures[executor.submit(sparql_query, client, query)] = chunk\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc='Relation batches'):\n",
    "        try:\n",
    "            res = fut.result(timeout=60)\n",
    "            for b in res['results']['bindings']:\n",
    "                s = b['s']['value'].split('/')[-1]\n",
    "                p = b['p']['value'].split('/')[-1]\n",
    "                o = b['o']['value'].split('/')[-1]\n",
    "                if s in all_entities and o in all_entities:\n",
    "                    add_triplet(triplets_out, triplets_out_seen, s, p, o)\n",
    "                    add_triplet(triplets_in,  triplets_in_seen,  o, p, s)\n",
    "                    relation_ids.add(p)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Relation chunk error {futures[fut]}: {e}\")\n",
    "\n",
    "# 3️⃣ Batch fetch all transitive property edges (deduped)\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {}\n",
    "    for prop in TRANSITIVE_PROPS:\n",
    "        for chunk in chunk_list(sorted(all_entities), TRANS_CHUNK_SIZE):\n",
    "            vals = ' '.join(f'wd:{e}' for e in chunk)\n",
    "            # Build path: P31 => one-step instanceOf then subclassOf*, P279 => subclassOf+, others => prop+\n",
    "            if prop == 'P31':\n",
    "                path_clause = '?s wdt:P31/wdt:P279* ?o .'\n",
    "            elif prop == 'P279':\n",
    "                path_clause = '?s wdt:P279+ ?o .'\n",
    "            else:\n",
    "                path_clause = f'?s wdt:{prop}+ ?o .'\n",
    "            query = f\"\"\"\n",
    "SELECT ?s ?o WHERE {{\n",
    "  VALUES ?s {{ {vals} }}\n",
    "  {path_clause}\n",
    "}}\"\"\"\n",
    "            client = SPARQLWrapper('https://query.wikidata.org/sparql')\n",
    "            client.setReturnFormat(JSON)\n",
    "            futures[executor.submit(sparql_query, client, query)] = (prop, chunk)\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc='Transitive batches'):\n",
    "        prop, chunk = futures[fut]\n",
    "        try:\n",
    "            res = fut.result(timeout=60)\n",
    "            for b in res['results']['bindings']:\n",
    "                s = b['s']['value'].split('/')[-1]\n",
    "                o = b['o']['value'].split('/')[-1]\n",
    "                if s in all_entities and o in all_entities:\n",
    "                    add_triplet(triplets_out, triplets_out_seen, s, prop, o)\n",
    "                    add_triplet(triplets_in,  triplets_in_seen,  o, prop, s)\n",
    "                    relation_ids.add(prop)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Transitive '{prop}' chunk error {chunk}: {e}\")\n",
    "\n",
    "# 4️⃣ Write head-centric triplets\n",
    "with open(os.path.join(OUTPUT_DIR, 'triplet_h.json'), 'w') as fh:\n",
    "    for k, trips in tqdm(triplets_out.items(), desc='Writing triplet_h.json'):\n",
    "        fh.write(json.dumps({'key': k, 'triplets': trips}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 5️⃣ Write tail-centric triplets\n",
    "with open(os.path.join(OUTPUT_DIR, 'triplet_t.json'), 'w') as ft:\n",
    "    for k, trips in tqdm(triplets_in.items(), desc='Writing triplet_t.json'):\n",
    "        ft.write(json.dumps({'key': k, 'triplets': trips}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 6️⃣ Write relation IDs\n",
    "with open(os.path.join(OUTPUT_DIR, 'relation.txt'), 'w') as fr:\n",
    "    for pid in tqdm(sorted(relation_ids), desc='Writing relation.txt'):\n",
    "        fr.write(pid + '\\n')\n",
    "\n",
    "print('Extraction complete with adjusted ancestor handling.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b81a66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86eee1b7",
   "metadata": {},
   "source": [
    "## Query the relation description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82de9167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  20%|█▉        | 100/501 [00:32<02:33,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P1408, retry 1/5 in 2.9s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  21%|██        | 105/501 [00:34<02:02,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P1427, retry 1/5 in 2.5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  21%|██        | 105/501 [00:34<02:02,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P1431, retry 1/5 in 2.8s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  22%|██▏       | 112/501 [00:37<02:04,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P1454: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  23%|██▎       | 113/501 [00:37<02:45,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P1427: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  23%|██▎       | 116/501 [00:38<02:03,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P1431, retry 2/5 in 4.2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  23%|██▎       | 117/501 [00:39<01:45,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P1531, retry 1/5 in 2.2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  24%|██▍       | 121/501 [00:41<03:13,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P155, retry 1/5 in 2.9s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  25%|██▍       | 123/501 [00:42<03:19,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P1557: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  26%|██▌       | 128/501 [00:43<01:39,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P1571: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  26%|██▋       | 132/501 [00:44<01:36,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P1589: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  27%|██▋       | 133/501 [00:45<02:34,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P155: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  27%|██▋       | 135/501 [00:45<02:05,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P16: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  30%|██▉       | 150/501 [00:50<01:58,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P1716: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  31%|███       | 156/501 [00:52<01:42,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P179: HTTP Error 429: Too Many Requests\n",
      "[403] Forbidden for P180, retry 1/5 in 2.4s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  32%|███▏      | 161/501 [00:53<01:33,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P1880: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  33%|███▎      | 167/501 [00:55<01:43,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P199, retry 1/5 in 2.1s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  34%|███▎      | 168/501 [00:56<02:36,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P180: HTTP Error 429: Too Many Requests\n",
      "Error fetching P1990: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  35%|███▌      | 177/501 [00:59<02:01,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P2079: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  36%|███▋      | 182/501 [01:00<01:54,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P2175: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  37%|███▋      | 183/501 [01:01<01:41,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching P2176: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  37%|███▋      | 184/501 [01:01<01:20,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P22, retry 1/5 in 2.3s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  38%|███▊      | 191/501 [01:03<01:57,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P2388, retry 1/5 in 2.4s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  42%|████▏     | 210/501 [01:09<00:57,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P2632, retry 1/5 in 2.4s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  42%|████▏     | 210/501 [01:10<00:57,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P2647, retry 1/5 in 2.8s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  43%|████▎     | 215/501 [01:11<01:57,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P2674, retry 1/5 in 2.9s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  45%|████▍     | 223/501 [01:15<02:24,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P277, retry 1/5 in 3.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  45%|████▌     | 227/501 [01:16<01:21,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P2821, retry 1/5 in 2.9s...\n",
      "[403] Forbidden for P2822, retry 1/5 in 2.5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  47%|████▋     | 233/501 [01:18<01:36,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P277, retry 2/5 in 4.4s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  47%|████▋     | 237/501 [01:21<02:17,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P2821, retry 2/5 in 4.6s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  51%|█████     | 256/501 [01:29<01:47,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P3137, retry 1/5 in 2.5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  53%|█████▎    | 266/501 [01:32<00:55,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P3137, retry 2/5 in 4.1s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  55%|█████▍    | 274/501 [01:34<01:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P360, retry 1/5 in 2.1s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  56%|█████▌    | 279/501 [01:37<01:32,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P3712, retry 1/5 in 2.3s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  56%|█████▋    | 282/501 [01:38<01:31,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P3730, retry 1/5 in 2.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  58%|█████▊    | 290/501 [01:40<01:07,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P3938, retry 1/5 in 2.9s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  62%|██████▏   | 312/501 [01:48<01:31,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P427, retry 1/5 in 2.6s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  63%|██████▎   | 314/501 [01:48<00:59,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P4424, retry 1/5 in 2.3s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  64%|██████▍   | 320/501 [01:50<00:59,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P457, retry 1/5 in 2.2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  76%|███████▌  | 381/501 [02:10<00:24,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P607, retry 1/5 in 2.5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  83%|████████▎ | 415/501 [02:20<00:30,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P710, retry 1/5 in 3.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  84%|████████▍ | 420/501 [02:22<00:27,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P737, retry 1/5 in 2.1s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  84%|████████▍ | 422/501 [02:22<00:24,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P747, retry 1/5 in 2.8s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  86%|████████▌ | 429/501 [02:26<00:31,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P737, retry 2/5 in 4.2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  86%|████████▋ | 433/501 [02:27<00:25,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P787, retry 1/5 in 3.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions:  98%|█████████▊| 489/501 [02:44<00:04,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403] Forbidden for P924, retry 1/5 in 2.1s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching descriptions: 100%|██████████| 501/501 [02:47<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written descriptions for 501 properties to kg/relation_descriptions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from urllib.error import HTTPError\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = 'kg/relation.txt'\n",
    "OUTPUT_FILE = 'kg/relation_descriptions.jsonl'\n",
    "SPARQL_ENDPOINT = 'https://query.wikidata.org/sparql'\n",
    "SLEEP_BETWEEN = 0.1                # seconds between queries to respect endpoint\n",
    "MAX_RETRIES = 5                    # retry attempts for 403/405 errors\n",
    "BASE_BACKOFF = 2                   # base seconds for exponential backoff\n",
    "MAX_WORKERS = 5                    # parallel threads for fetching\n",
    "\n",
    "# Read all property IDs\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    pids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Function to fetch description with retry on 403/405\n",
    "def get_description(pid):\n",
    "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    query = f\"\"\"\n",
    "SELECT ?desc WHERE {{\n",
    "  wd:{pid} schema:description ?desc .\n",
    "  FILTER(LANG(?desc) = 'en')\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            result = sparql.query().convert()\n",
    "            bindings = result.get('results', {}).get('bindings', [])\n",
    "            return bindings[0]['desc']['value'] if bindings else ''\n",
    "        except HTTPError as e:\n",
    "            code = getattr(e, 'code', None)\n",
    "            if code in (403, 405) and attempt < MAX_RETRIES:\n",
    "                backoff = BASE_BACKOFF * (2 ** (attempt - 1)) + random.random()\n",
    "                tqdm.write(f\"[{code}] Forbidden for {pid}, retry {attempt}/{MAX_RETRIES} in {backoff:.1f}s...\")\n",
    "                time.sleep(backoff)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE) or '.', exist_ok=True)\n",
    "\n",
    "# Fetch descriptions in parallel and write to file\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:\n",
    "    futures = {}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        for pid in pids:\n",
    "            futures[executor.submit(get_description, pid)] = pid\n",
    "        for fut in tqdm(as_completed(futures), total=len(pids), desc='Fetching descriptions'):\n",
    "            pid = futures[fut]\n",
    "            try:\n",
    "                desc = fut.result()\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error fetching {pid}: {e}\")\n",
    "                desc = ''\n",
    "            record = {'p_id': pid, 'description': desc}\n",
    "            out.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "print(f\"Written descriptions for {len(pids)} properties to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e0551",
   "metadata": {},
   "source": [
    "## merge two relation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "830a7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 501 + 825 → 955 unique records into kg/merged_relation_descriptions.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "INPUT_FILE_1 = 'kg/relation_descriptions.jsonl'\n",
    "INPUT_FILE_2 = 'kg/relation_descriptions2.jsonl'\n",
    "OUTPUT_FILE = 'kg/merged_relation_descriptions.jsonl'\n",
    "\n",
    "def load_descriptions(path):\n",
    "    \"\"\"从 JSONL 文件中加载 {p_id: description} 映射。\"\"\"\n",
    "    descs = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                pid = rec.get('p_id')\n",
    "                desc = rec.get('description', '')\n",
    "                if pid:\n",
    "                    # 如果同一个 p_id 在两个文件中都出现，以第二个文件中的为准：\n",
    "                    descs[pid] = desc\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return descs\n",
    "\n",
    "def merge_and_write(path1, path2, out_path):\n",
    "    # 1. 读取两个文件\n",
    "    descs1 = load_descriptions(path1)\n",
    "    descs2 = load_descriptions(path2)\n",
    "\n",
    "    # 2. 合并：先放入 descs1，再用 descs2 覆盖（如果有重复）\n",
    "    merged = {**descs1, **descs2}\n",
    "\n",
    "    # 3. 写回 JSONL\n",
    "    with open(out_path, 'w', encoding='utf-8') as out:\n",
    "        for pid, desc in merged.items():\n",
    "            record = {'p_id': pid, 'description': desc}\n",
    "            out.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Merged {len(descs1)} + {len(descs2)} → {len(merged)} unique records into {out_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_and_write(INPUT_FILE_1, INPUT_FILE_2, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4194d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversed triplets have been saved to /home/zho1rng/oven_train/dataset/wikidata_subgraph_v1/triplet_t1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 输入文件路径\n",
    "input_file = '/home/zho1rng/oven_train/dataset/wikidata_subgraph_v1/triplet_t.jsonl'\n",
    "output_file = '/home/zho1rng/oven_train/dataset/wikidata_subgraph_v1/triplet_t1.jsonl'\n",
    "\n",
    "# 翻转 triplets 的 head 和 tail\n",
    "def reverse_triplets(data):\n",
    "    reversed_triplets = []\n",
    "    for head, relation, tail in data['triplets']:\n",
    "        reversed_triplets.append([tail, relation, head])\n",
    "    return {'key': data['key'], 'triplets': reversed_triplets}\n",
    "\n",
    "# 读取输入文件并翻转triplets\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    for line in infile:\n",
    "        data = json.loads(line.strip())\n",
    "        reversed_data = reverse_triplets(data)\n",
    "        outfile.write(json.dumps(reversed_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Reversed triplets have been saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
